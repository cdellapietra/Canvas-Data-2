{"cells":[{"cell_type":"code","source":["%%bash\n","pip install instructure-dap-client"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e6a20fe5-a69e-43e8-aa38-8d4fc09f155b","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T18:25:39.5575793Z","session_start_time":null,"execution_start_time":"2023-08-07T18:25:40.0337347Z","execution_finish_time":"2023-08-07T18:25:46.616434Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"31909701-ebd2-4a1a-a16f-13bdf1502be0"},"text/plain":"StatementMeta(, e6a20fe5-a69e-43e8-aa38-8d4fc09f155b, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting instructure-dap-client\n  Downloading instructure_dap_client-0.3.10-py3-none-any.whl (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting SQLAlchemy[asyncio]>=2.0.16\n  Downloading SQLAlchemy-2.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting asyncpg>=0.27.0\n  Downloading asyncpg-0.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting aiofiles>=23.1.0\n  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\nCollecting types-aiofiles>=23.1.0\n  Downloading types_aiofiles-23.1.0.5-py3-none-any.whl (8.8 kB)\nRequirement already satisfied: aiohttp>=3.8.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from instructure-dap-client) (3.8.4)\nCollecting json-strong-typing>=0.2.7\n  Downloading json_strong_typing-0.2.7-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting orjson>=3.9.1\n  Downloading orjson-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting PyJWT>=2.7.0\n  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (1.3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (6.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (1.3.1)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (2.1.1)\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (22.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp>=3.8.4->instructure-dap-client) (1.8.2)\nRequirement already satisfied: jsonschema>=4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from json-strong-typing>=0.2.7->instructure-dap-client) (4.17.3)\nRequirement already satisfied: typing-extensions>=4.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=2.0.16->instructure-dap-client) (4.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=2.0.16->instructure-dap-client) (2.0.2)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jsonschema>=4.17->json-strong-typing>=0.2.7->instructure-dap-client) (0.19.3)\nRequirement already satisfied: idna>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.4->instructure-dap-client) (3.4)\nInstalling collected packages: types-aiofiles, SQLAlchemy, PyJWT, orjson, asyncpg, aiofiles, json-strong-typing, instructure-dap-client\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.9\n    Uninstalling SQLAlchemy-2.0.9:\n      Successfully uninstalled SQLAlchemy-2.0.9\n  Attempting uninstall: PyJWT\n    Found existing installation: PyJWT 2.6.0\n    Uninstalling PyJWT-2.6.0:\n      Successfully uninstalled PyJWT-2.6.0\nSuccessfully installed PyJWT-2.8.0 SQLAlchemy-2.0.19 aiofiles-23.1.0 asyncpg-0.28.0 instructure-dap-client-0.3.10 json-strong-typing-0.2.7 orjson-3.9.3 types-aiofiles-23.1.0.5\n"]}],"execution_count":2,"metadata":{"cellStatus":"{\"Corey Della Pietra\":{\"queued_time\":\"2023-08-07T18:25:39.5575793Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-07T18:25:40.0337347Z\",\"execution_finish_time\":\"2023-08-07T18:25:46.616434Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":true}},"id":"e6db6044-8b8f-4764-b680-e012bac859de"},{"cell_type":"code","source":["from datetime import datetime\n","import subprocess\n","import multiprocessing\n","\n","def run_bash_command(command):\n","    subprocess.run(command, shell=True)\n","\n","api_url = \"https://api-gateway.instructure.com\"\n","client_id = \"us-west-2#8cb98924-70a9-49b1-bfe9-393e6ae111d5\"\n","client_secret = \"3zlv-vTSxyZvnR7grsv56PYBnAgy_N4qug1DP-YIzQs\"\n","\n","# define tables to be extracted from Canvas\n","tables = [\n","    \"users\"\n","]\n","\n","commands = []\n","\n","# create string timestamp to serve as directory\n","now = datetime.now()\n","date_ts = now.strftime(\"%Y%m%d%H%M%S\")\n","\n","# build bash command for the incremental load of each table\n","for table in tables:\n","\n","    # retrieve most-recent load timestamp for 'since' parameter\n","    query  = f\"SELECT MAX(ts) FROM {table}\"\n","    result = spark.sql(query).collect()\n","    #since  = result[0][0]\n","    since = \"2023-05-03T20:24:58\"\n","\n","    # build the command string\n","    file_path = f\"/lakehouse/default/Files/Canvas/{table}/{date_ts}\"\n","    commands.append(f\"dap --base-url {api_url} --client-id {client_id} --client-secret {client_secret} incremental --format parquet --output-directory {file_path} --table {table} --namespace canvas --since {since}\")\n","\n","\n","# create and start a separate process for each command\n","processes = [multiprocessing.Process(target=run_bash_command, args=(cmd,)) for cmd in commands]\n","\n","for process in processes:\n","    process.start()\n","\n","# wait for all processes to complete\n","for process in processes:\n","    process.join()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e6a20fe5-a69e-43e8-aa38-8d4fc09f155b","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T18:25:39.6405252Z","session_start_time":null,"execution_start_time":"2023-08-07T18:25:47.1044109Z","execution_finish_time":"2023-08-07T18:26:46.1717854Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":3,"UNKNOWN":0,"FAILED":0},"jobs":[{"displayName":"collect at /tmp/ipykernel_8086/1581391420.py:28","dataWritten":0,"dataRead":84,"rowCount":1,"usageDescription":"","jobId":16,"name":"collect at /tmp/ipykernel_8086/1581391420.py:28","description":"Job group for statement 5:\nfrom datetime import datetime\nimport subprocess\nimport multiprocessing\n\ndef run_bash_command(command):\n    subprocess.run(command, shell=True)\n\napi_url = \"https://api-gateway.instructure.com\"\nclient_id = \"us-west-2#8cb98924-70a9-49b1-bfe9-393e6ae111d5\"\nclient_secret = \"3zlv-vTSxyZvnR7grsv56PYBnAgy_N4qug1DP-YIzQs\"\n\n# define tables to be extracted from Canvas\ntables = [\n    \"users\"\n]\n\ncommands = []\n\n# create string timestamp to serve as directory\nnow = datetime.now()\ndate_ts = now.strftime(\"%Y%m%d%H%M%S\")\n\n# build bash command for the incremental load of each table\nfor table in tables:\n\n    # retrieve most-recent load timestamp for 'since' parameter\n    query  = f\"SELECT MAX(ts) FROM {table}\"\n    result = spark.sql(query).collect()\n    #since  = result[0][0]\n    since = \"2023-05-03T20:24:58\"\n\n    # build the command string\n    file_path = f\"/lakehouse/default/Files/Canvas/{table}/{date_ts}\"\n    commands.append(f\"dap --base-url {api_url} --client-id {client_id} --client-secret {client_secret} incremental --forma...","submissionTime":"2023-08-07T18:25:48.372GMT","completionTime":"2023-08-07T18:25:48.397GMT","stageIds":[27,26],"jobGroup":"5","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8086/1581391420.py:28","dataWritten":84,"dataRead":5857,"rowCount":16,"usageDescription":"","jobId":15,"name":"collect at /tmp/ipykernel_8086/1581391420.py:28","description":"Job group for statement 5:\nfrom datetime import datetime\nimport subprocess\nimport multiprocessing\n\ndef run_bash_command(command):\n    subprocess.run(command, shell=True)\n\napi_url = \"https://api-gateway.instructure.com\"\nclient_id = \"us-west-2#8cb98924-70a9-49b1-bfe9-393e6ae111d5\"\nclient_secret = \"3zlv-vTSxyZvnR7grsv56PYBnAgy_N4qug1DP-YIzQs\"\n\n# define tables to be extracted from Canvas\ntables = [\n    \"users\"\n]\n\ncommands = []\n\n# create string timestamp to serve as directory\nnow = datetime.now()\ndate_ts = now.strftime(\"%Y%m%d%H%M%S\")\n\n# build bash command for the incremental load of each table\nfor table in tables:\n\n    # retrieve most-recent load timestamp for 'since' parameter\n    query  = f\"SELECT MAX(ts) FROM {table}\"\n    result = spark.sql(query).collect()\n    #since  = result[0][0]\n    since = \"2023-05-03T20:24:58\"\n\n    # build the command string\n    file_path = f\"/lakehouse/default/Files/Canvas/{table}/{date_ts}\"\n    commands.append(f\"dap --base-url {api_url} --client-id {client_id} --client-secret {client_secret} incremental --forma...","submissionTime":"2023-08-07T18:25:48.173GMT","completionTime":"2023-08-07T18:25:48.352GMT","stageIds":[25],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_8086/1581391420.py:28","dataWritten":0,"dataRead":3715,"rowCount":7,"usageDescription":"","jobId":14,"name":"collect at /tmp/ipykernel_8086/1581391420.py:28","description":"Delta: Job group for statement 5:\nfrom datetime import datetime\nimport subprocess\nimport multiprocessing\n\ndef run_bash_command(command):\n    subprocess.run(command, shell=True)\n\napi_url = \"https://api-gateway.instructure.com\"\nclient_id = \"us-west-2#8cb98924-70a9-49b1-bfe9-393e6ae111d5\"\nclient_secret = \"3zlv-vTSxyZvnR7grsv56PYBnAgy_N4qug1DP-YIzQs\"\n\n# define tables to be extracted from Canvas\ntables = [\n    \"users\"\n]\n\ncommands = []\n\n# create string timestamp to serve as directory\nnow = datetime.now()\ndate_ts = now.strftime(\"%Y%m%d%H%M%S\")\n\n# build bash command for the incremental load of each table\nfor table in tables:\n\n    # retrieve most-recent load timestamp for 'since' parameter\n    query  = f\"SELECT MAX(ts) FROM {table}\"\n    result = spark.sql(query).collect()\n    #since  = result[0][0]\n    since = \"2023-05-03T20:24:58\"\n\n    # build the command string\n    file_path = f\"/lakehouse/default/Files/Canvas/{table}/{date_ts}\"\n    commands.append(f\"dap --base-url {api_url} --client-id {client_id} --client-secret {client_secret} incremental --forma...: Filtering files for query","submissionTime":"2023-08-07T18:25:47.900GMT","completionTime":"2023-08-07T18:25:48.092GMT","stageIds":[24,23],"jobGroup":"5","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"245ebe11-d024-43ef-80db-52ff41c83eb8"},"text/plain":"StatementMeta(, e6a20fe5-a69e-43e8-aa38-8d4fc09f155b, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2023-08-07 18:25:51,040 - INFO - Query started with job ID: 393238c4-58d5-4b6c-aebb-a10dd92a41f9\n2023-08-07 18:25:51,040 - INFO - Query job still in status: waiting. Checking again in 5 seconds...\n2023-08-07 18:25:56,594 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:01,958 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:07,840 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:13,378 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:18,864 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:24,851 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:30,398 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:35,859 - INFO - Query job still in status: running. Checking again in 5 seconds...\n2023-08-07 18:26:43,793 - INFO - Files from server downloaded to folder: /lakehouse/default/Files/Canvas/users/20230807182547/job_393238c4-58d5-4b6c-aebb-a10dd92a41f9\n2023-08-07 18:26:43,794 - INFO - Incremental query results have been successfully retrieved:\n{\"id\": \"393238c4-58d5-4b6c-aebb-a10dd92a41f9\", \"status\": \"complete\", \"expires_at\": \"2023-08-08T18:25:50Z\", \"objects\": [{\"id\": \"393238c4-58d5-4b6c-aebb-a10dd92a41f9/part-00000-df15395d-5573-4e84-967a-607203102946-c000.gz.parquet\"}], \"schema_version\": 2, \"since\": \"2023-05-03T20:24:58Z\", \"until\": \"2023-08-07T18:06:17Z\"}\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Corey Della Pietra\":{\"queued_time\":\"2023-08-07T18:25:39.6405252Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-07T18:25:47.1044109Z\",\"execution_finish_time\":\"2023-08-07T18:26:46.1717854Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":true},"id":"c4d122f1-8cea-45fb-b8ec-d78b497514e5"},{"cell_type":"code","source":["# perform a merge between each staging table and delta table\n","for table in tables:\n","\n","    # transform the incremental data and store to a temporary table\n","    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n","    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n","    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n","\n","    updates = []\n","    inserts = []\n","\n","    # dynamically build sql query components \n","    for value in df.select(\"value.*\").columns:\n","        \n","        updates.append(f\"t.{value} = s.{value}\")\n","        inserts.append(value)\n","\n","    update  = \"\\n\\t\\t\\t, \".join(updates)\n","    insert  = \"\\n\\t\\t\\t, \".join(inserts)\n","    insert2 = \"\\n\\t\\t\\t, s.\".join(inserts)\n","\n","    sql = f\"\"\"\n","        MERGE INTO {table} t\n","        USING {table}_stage s\n","        ON t.id = s.id\n","        WHEN MATCHED AND s.action = 'U' THEN \n","        UPDATE SET\n","            {update}\n","            , t.ts = s.ts\n","        WHEN MATCHED AND s.action = 'D' THEN\n","        DELETE \n","        WHEN NOT MATCHED AND s.action = 'I' THEN\n","        INSERT (\n","            id\n","            , {insert}\n","            , ts\n","        ) VALUES (\n","            s.id\n","            , {insert2}\n","            , s.ts\n","        )\n","    \"\"\"\n","\n","    # execute merge and show results\n","    display(spark.sql(sql))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e6a20fe5-a69e-43e8-aa38-8d4fc09f155b","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-07T18:25:39.74316Z","session_start_time":null,"execution_start_time":"2023-08-07T18:26:46.5847502Z","execution_finish_time":"2023-08-07T18:26:56.7033277Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":13,"UNKNOWN":0,"FAILED":0},"jobs":[{"displayName":"getRowsInJsonString at Display.scala:403","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":30,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...","submissionTime":"2023-08-07T18:26:54.952GMT","completionTime":"2023-08-07T18:26:54.980GMT","stageIds":[51],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4626,"rowCount":50,"usageDescription":"","jobId":29,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Compute snapshot for version: 9","submissionTime":"2023-08-07T18:26:54.750GMT","completionTime":"2023-08-07T18:26:54.796GMT","stageIds":[48,49,50],"jobGroup":"6","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4626,"dataRead":16880,"rowCount":73,"usageDescription":"","jobId":28,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Compute snapshot for version: 9","submissionTime":"2023-08-07T18:26:53.881GMT","completionTime":"2023-08-07T18:26:54.727GMT","stageIds":[46,47],"jobGroup":"6","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":16880,"dataRead":22636,"rowCount":46,"usageDescription":"","jobId":27,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Compute snapshot for version: 9","submissionTime":"2023-08-07T18:26:53.478GMT","completionTime":"2023-08-07T18:26:53.715GMT","stageIds":[45],"jobGroup":"6","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":13425,"dataRead":3110,"rowCount":30,"usageDescription":"","jobId":26,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Writing merged data full update","submissionTime":"2023-08-07T18:26:52.488GMT","completionTime":"2023-08-07T18:26:52.776GMT","stageIds":[42,43,44,41],"jobGroup":"6","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":3,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":3110,"dataRead":10591,"rowCount":45,"usageDescription":"","jobId":25,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Writing merged data full update","submissionTime":"2023-08-07T18:26:51.734GMT","completionTime":"2023-08-07T18:26:52.427GMT","stageIds":[38,39,40],"jobGroup":"6","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":5156,"dataRead":10631,"rowCount":30,"usageDescription":"","jobId":24,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Writing merged data full update","submissionTime":"2023-08-07T18:26:51.330GMT","completionTime":"2023-08-07T18:26:51.523GMT","stageIds":[37],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":2523,"dataRead":13150,"rowCount":15,"usageDescription":"","jobId":23,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Writing merged no shuffle data","submissionTime":"2023-08-07T18:26:51.267GMT","completionTime":"2023-08-07T18:26:51.907GMT","stageIds":[36],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"run at ForkJoinTask.java:1402","dataWritten":5435,"dataRead":3296,"rowCount":30,"usageDescription":"","jobId":22,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Writing merged data full update","submissionTime":"2023-08-07T18:26:51.171GMT","completionTime":"2023-08-07T18:26:51.255GMT","stageIds":[35],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":501,"rowCount":1,"usageDescription":"","jobId":21,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T18:26:49.308GMT","completionTime":"2023-08-07T18:26:50.841GMT","stageIds":[33,34],"jobGroup":"6","status":"SUCCEEDED","numTasks":201,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":501,"dataRead":5768,"rowCount":16,"usageDescription":"","jobId":20,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T18:26:48.799GMT","completionTime":"2023-08-07T18:26:49.184GMT","stageIds":[32],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":3715,"rowCount":7,"usageDescription":"","jobId":18,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...: Finding touched files - low shuffle merge","submissionTime":"2023-08-07T18:26:47.877GMT","completionTime":"2023-08-07T18:26:48.063GMT","stageIds":[30,29],"jobGroup":"6","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":17,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\n# perform a merge between each staging table and delta table\nfor table in tables:\n\n    # transform the incremental data and store to a temporary table\n    df = spark.read.parquet(f\"Files/Canvas/{table}/{date_ts}/*/*.parquet\")\n    flat_df = df.select(\"key.*\", \"value.*\", \"meta.*\")\n    flat_df.createOrReplaceTempView(f\"{table}_stage\")\n\n    updates = []\n    inserts = []\n\n    # dynamically build sql query components \n    for value in df.select(\"value.*\").columns:\n        \n        updates.append(f\"t.{value} = s.{value}\")\n        inserts.append(value)\n\n    update  = \"\n\t\t\t, \".join(updates)\n    insert  = \"\n\t\t\t, \".join(inserts)\n    insert2 = \"\n\t\t\t, s.\".join(inserts)\n\n    sql = f\"\"\"\n        MERGE INTO {table} t\n        USING {table}_stage s\n        ON t.id = s.id\n        WHEN MATCHED AND s.action = 'U' THEN \n        UPDATE SET\n            {update}\n            , t.ts = s.ts\n        WHEN MATCHED AND s.action = 'D' THEN\n        DELETE \n        WHEN NOT MATCHED AND s.action = 'I' THEN\n        INSERT (\n          ...","submissionTime":"2023-08-07T18:26:46.798GMT","completionTime":"2023-08-07T18:26:46.929GMT","stageIds":[28],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9ba1bb3a-05cd-4fc8-94cd-f60e10788f02"},"text/plain":"StatementMeta(, e6a20fe5-a69e-43e8-aa38-8d4fc09f155b, 6, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"b0640ca1-a9f9-423d-8f66-4e0243504673","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, b0640ca1-a9f9-423d-8f66-4e0243504673)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Corey Della Pietra\":{\"queued_time\":\"2023-08-07T18:25:39.74316Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-07T18:26:46.5847502Z\",\"execution_finish_time\":\"2023-08-07T18:26:56.7033277Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false},"id":"2533582f-2c9e-40a2-8c79-238aefe904dd"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{"trident":{"lakehouse":{"known_lakehouses":"[{\"id\":\"98f791bc-601f-4863-816d-baccca04f46f\"}]","default_lakehouse":"98f791bc-601f-4863-816d-baccca04f46f"}},"synapse_widget":{"token":"cd514afa-8670-46b8-a84e-dbc4d3474eda","state":{"b0640ca1-a9f9-423d-8f66-4e0243504673":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"15","1":"15","2":"0","3":"0","index":1}],"schema":[{"key":"0","name":"num_affected_rows","type":"bigint"},{"key":"1","name":"num_updated_rows","type":"bigint"},{"key":"2","name":"num_deleted_rows","type":"bigint"},{"key":"3","name":"num_inserted_rows","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"b0640ca1-a9f9-423d-8f66-4e0243504673":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"15","1":"15","2":"0","3":"0","index":1}],"schema":[{"key":"0","name":"num_affected_rows","type":"bigint"},{"key":"1","name":"num_updated_rows","type":"bigint"},{"key":"2","name":"num_deleted_rows","type":"bigint"},{"key":"3","name":"num_inserted_rows","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"default_lakehouse":"98f791bc-601f-4863-816d-baccca04f46f","known_lakehouses":[{"id":"98f791bc-601f-4863-816d-baccca04f46f"}],"default_lakehouse_name":"FAULH","default_lakehouse_workspace_id":"b4c38c86-f275-4e27-b17d-7ff04c52ce47"}}},"nbformat":4,"nbformat_minor":5}